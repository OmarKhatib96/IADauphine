{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PDS2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFlsknEbFtPB",
        "outputId": "88607a3d-54dc-4998-86a3-c3cea6478ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "from math import tanh\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from scipy import optimize\n",
        "#Box constraints in 3 methods : 1- projected gradient descent(clipps the coordinates to be inside the box)\n",
        "                               #2- the xi are not clipped , but instead we clipp the gradient: we replace f(x+delta) by f(min(max(x+delta,0),1))\n",
        "                               #3-but the best is the one below with the variable_change tanh\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Copie de Projet SD - Partie 1.ipynb\n",
        "Automatically generated by Colaboratory.\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1m4u7cVrOUqmZNV0ZjOPgj5y9yeYvqI_s\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "# Packages to load CIFAR10\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# Packages to define Neural Network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Package to define an optmizer\n",
        "import torch.optim as optim\n",
        "#import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "delta=[0,0,0]\n",
        "k=0.09\n",
        "\n",
        "# correct solution:\n",
        "def softmax(x,i):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    output=e_x / e_x.sum(axis=0) # only difference\n",
        "    return output[i]\n",
        "\n",
        "\n",
        "#Define a CNN\n",
        "'''\n",
        "A neural network is a function F(x)=y that accepts an input\n",
        "x belonging to Rn and produces an output y belonging to Rm,\n",
        "y is a probability distribution that x has class i\n",
        "'''\n",
        "\n",
        "\n",
        "def norm_p(delta, p):\n",
        "    \"First-pass implementation of p-norm.\"\n",
        "    return (np.abs(delta)**p).sum() ** (1./p)\n",
        "\n",
        "\n",
        "def variable_change(w,x,i):#ensures that x+delta belongs to [0,1]\n",
        "\n",
        "  delta=0.5*(tanh(w[i])+1)-x[i] #this is equals to delta\n",
        "  return delta\n",
        "  #since -1<=tanh(w)<=1, it follows 0<=x_i+delta_i<=1, so this solution will automatically be valid, we can think of this method as,a clipped descent gradient that eliminates \n",
        "  #problem of getting stuck in extreme regions\n",
        "\n",
        "\n",
        "def objectif_function(my_array,i,t):#this is f\n",
        "\n",
        "  if(i!=t):\n",
        "    return max(softmax(my_array,i)-softmax(my_array,t),0.12)\n",
        "  else:\n",
        "    return k\n",
        "\n",
        "  \n",
        "#what we're trying to do is to minimize D(x,x+delta)\n",
        "  #such that f(x+delta)<=0 and x+delta belongs to [0,1]\n",
        "  \n",
        "\n",
        "def function_to_minimize(delta,p,x,c,i,t):\n",
        "  return norm_p(delta,p)+c*objectif_function(x+delta,i,t)\n",
        "\n",
        "\n",
        "def attack_l2(w,x,c,t):\n",
        "  for i in range(len(x)):\n",
        "    delta[i]=variable_change(w,x,i)\n",
        "    w[i]=function_to_minimize(delta[i],2,x,c,i,t)\n",
        "  return w[i]\n",
        "\n",
        "\n",
        "  \n",
        "x0=np.array([0.04,0.1,1.5])\n",
        "w0=np.array([0.05,12,0.8])\n",
        "c=0.1\n",
        "result = optimize.minimize(attack_l2,x0,args=(w0,c,1),method='SLSQP',\n",
        "                           options={'maxiter':100})\n",
        "\n",
        "\n",
        "print(result)\n",
        "\n",
        "print(np.array(delta)+result['x'])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     fun: 0.812\n",
            "     jac: array([16173143.12556883, 16173143.12556883, 16173142.62668798])\n",
            " message: 'Optimization terminated successfully.'\n",
            "    nfev: 61\n",
            "     nit: 4\n",
            "    njev: 4\n",
            "  status: 0\n",
            " success: True\n",
            "       x: array([ 0.062, 12.009,  0.812])\n",
            "[0.012 0.009 0.012]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
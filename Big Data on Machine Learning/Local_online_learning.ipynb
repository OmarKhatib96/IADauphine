{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Local online learning",
      "dashboards": [],
      "language": "python",
      "widgets": {},
      "notebookOrigID": 2370986528414860
    },
    "colab": {
      "name": "Local online learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cb667d2a-3b5f-410e-aaf4-dd439732c51e"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_PJ4WGg1toG",
        "outputId": "1efa68e1-16f4-4aae-8f44-37a863dd21dc"
      },
      "source": [
        "!pip install --upgrade pip -q\n",
        "!pip install progressbar -q\n",
        "!pip install memory_profiler -q\n",
        "!pip install --upgrade pandas>=1.2 -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 21.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184kB 8.2MB/s eta 0:00:01\r\u001b[K     |████                            | 194kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 245kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 266kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 276kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 296kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 307kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 317kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 327kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 337kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 348kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 358kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 368kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 378kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 389kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 399kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 409kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 419kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 440kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 450kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 460kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 471kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 481kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 491kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 501kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 512kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 522kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 532kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 542kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 552kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 563kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 573kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 583kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 593kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 604kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 614kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 624kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 634kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 645kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 655kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 665kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 675kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 686kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 696kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 706kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 716kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 727kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 737kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 747kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 757kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 768kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 778kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 788kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 798kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 808kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 819kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 829kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 839kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 849kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 860kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 870kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 880kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 890kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 901kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 911kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 921kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 931kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 942kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 952kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 962kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 972kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 983kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 993kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.0MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.0MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.0MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.2MB/s \n",
            "\u001b[?25h  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "28d273c8-e128-4b73-9b5e-aa4eab0656df"
        },
        "id": "HKDvKTXg1toM"
      },
      "source": [
        "%load_ext memory_profiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b638cb6c-d45d-4a28-8839-bb1c5dad06f1"
        },
        "id": "tLgt10PR1toN"
      },
      "source": [
        "import urllib\n",
        "import tarfile\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import warnings\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import progressbar\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import roc_auc_score, log_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c53bbab7-6051-4983-96e3-793ee117588b"
        },
        "id": "hyQSYsL71toN"
      },
      "source": [
        "## Download Criteo  Display Advertising Challenge dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "af3c5721-fde1-4c56-8437-57d8e8bc5311"
        },
        "id": "Wb-muTCO1toO"
      },
      "source": [
        "# ProgressBar borrowed from https://stackoverflow.com/a/53643011/2015762\n",
        "class ProgressBar():\n",
        "    def __init__(self):\n",
        "        self.pbar = None\n",
        "\n",
        "    def __call__(self, block_num, block_size, total_size):\n",
        "        if not self.pbar:\n",
        "            self.pbar=progressbar.ProgressBar(maxval=total_size)\n",
        "            self.pbar.start()\n",
        "\n",
        "        downloaded = block_num * block_size\n",
        "        if downloaded < total_size:\n",
        "            self.pbar.update(downloaded)\n",
        "        else:\n",
        "            self.pbar.finish()\n",
        "\n",
        "\n",
        "def download_dataset(dataset_url, dataset_folder_path, compressed_dataset_path):\n",
        "    # Download dataset\n",
        "    os.makedirs(dataset_folder_path, exist_ok=True)\n",
        "    urllib.request.urlretrieve(dataset_url, compressed_dataset_path, ProgressBar())\n",
        "\n",
        "def extract_dataset(compressed_dataset_path, dataset_folder_path, dataset_path):\n",
        "    # Extract train.txt (dataset with labels) and readme\n",
        "    with tarfile.open(compressed_dataset_path, \"r\") as input_file:\n",
        "        input_file.extract('readme.txt', dataset_folder_path)\n",
        "        input_file.extract('train.txt', dataset_folder_path)\n",
        "        os.rename(os.path.join(dataset_folder_path, 'train.txt'), dataset_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1478b2f8-d38d-47c0-83da-60b9bc391b2b"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPbH1qml1toO",
        "outputId": "1df6223f-e6f2-4155-9705-5fb3f50f151b"
      },
      "source": [
        "dataset_url = \"https://criteostorage.blob.core.windows.net/criteo-research-datasets/kaggle-display-advertising-challenge-dataset.tar.gz\"\n",
        "dataset_folder_path = os.path.abspath('sync/data/criteo_dataset')\n",
        "compressed_dataset_path = os.path.join(dataset_folder_path, \"criteo_dataset.tar.gz\")\n",
        "dataset_path = os.path.join(dataset_folder_path, \"criteo_dataset.txt\")\n",
        "\n",
        "if not os.path.exists(compressed_dataset_path):\n",
        "    download_dataset(dataset_url, dataset_folder_path, compressed_dataset_path)\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    extract_dataset(compressed_dataset_path, dataset_folder_path, dataset_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "55660968-0843-4d77-a996-c0970110f7bc"
        },
        "id": "fCc0XZgS1toO"
      },
      "source": [
        "Quick look at the files we have downloaded.\n",
        "\n",
        "Within iPython notebook, we can execute bash command by prepending the cell with `!` and insert python variable into it with `{}`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99c7acfc-a685-4e49-a609-a409d6199f62"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0jlL7pR1toP",
        "outputId": "7109cab3-2e56-4d2d-82f3-e1dc0647a12b"
      },
      "source": [
        "!ls -alh {dataset_folder_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 15G\n",
            "drwxr-xr-x 2 root      root  4.0K Mar  5 14:54 .\n",
            "drwxr-xr-x 3 root      root  4.0K Mar  5 14:44 ..\n",
            "-rw-r--r-- 1 root      root  4.3G Mar  5 14:48 criteo_dataset.tar.gz\n",
            "-rw-r--r-- 1 293604138 staff  11G May 12  2014 criteo_dataset.txt\n",
            "-rw-r--r-- 1 293604138 staff 1.9K Aug 22  2014 readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99f480f9-5224-419b-904a-fc38429d97e6"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRJneGpn1toP",
        "outputId": "6b4ce356-89a6-476e-adc5-9fbf0cf253b1"
      },
      "source": [
        "!cat {dataset_folder_path}/readme.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        ------ Display Advertising Challenge ------\n",
            "\n",
            "Dataset: dac-v1\n",
            "\n",
            "This dataset contains feature values and click feedback for millions of display \n",
            "ads. Its purpose is to benchmark algorithms for clickthrough rate (CTR) prediction.\n",
            "It has been used for the Display Advertising Challenge hosted by Kaggle:\n",
            "https://www.kaggle.com/c/criteo-display-ad-challenge/\n",
            "\n",
            "===================================================\n",
            "\n",
            "Full description:\n",
            "\n",
            "This dataset contains 2 files:\n",
            "  train.txt\n",
            "  test.txt\n",
            "corresponding to the training and test parts of the data. \n",
            "\n",
            "====================================================\n",
            "\n",
            "Dataset construction:\n",
            "\n",
            "The training dataset consists of a portion of Criteo's traffic over a period\n",
            "of 7 days. Each row corresponds to a display ad served by Criteo and the first\n",
            "column is indicates whether this ad has been clicked or not.\n",
            "The positive (clicked) and negatives (non-clicked) examples have both been\n",
            "subsampled (but at different rates) in order to reduce the dataset size.\n",
            "\n",
            "There are 13 features taking integer values (mostly count features) and 26\n",
            "categorical features. The values of the categorical features have been hashed\n",
            "onto 32 bits for anonymization purposes. \n",
            "The semantic of these features is undisclosed. Some features may have missing values.\n",
            "\n",
            "The rows are chronologically ordered.\n",
            "\n",
            "The test set is computed in the same way as the training set but it \n",
            "corresponds to events on the day following the training period. \n",
            "The first column (label) has been removed.\n",
            "\n",
            "====================================================\n",
            "\n",
            "Format:\n",
            "\n",
            "The columns are tab separeted with the following schema:\n",
            "<label> <integer feature 1> ... <integer feature 13> <categorical feature 1> ... <categorical feature 26>\n",
            "\n",
            "When a value is missing, the field is just empty.\n",
            "There is no label field in the test set.\n",
            "\n",
            "====================================================\n",
            "\n",
            "Dataset assembled by Olivier Chapelle (o.chapelle@criteo.com)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "83209dbe-6666-43d0-8ab2-c9a7139ae4e3"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "rd1NfEi71toP",
        "outputId": "abda3aca-8b6d-43f7-9912-b2d4ad740583"
      },
      "source": [
        "label_columns = #\n",
        "integer_features = #\n",
        "categorical_features = #\n",
        "columns = label_columns + integer_features + categorical_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-2dc9ca8cb2ee>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    label_columns = #\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "97987163-55dc-4807-a4e8-0d5db8781629"
        },
        "id": "G__yxWni1toP"
      },
      "source": [
        "pd.read_csv(dataset_path, nrows=5, header=None, sep='\\t', names=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "48033b2b-08be-4490-b9c8-4d6c8d61b38d"
        },
        "id": "gGtGs8BF1toQ"
      },
      "source": [
        "## Reading data with memory constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "23587706-411e-487a-89b6-2423b7e389dc"
        },
        "id": "NjsXNMbs1toQ"
      },
      "source": [
        "We first create a toy dataset with \"only\" 1 million rows (out of 45 millions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c1015efa-7924-4f3a-8bac-0c0fc691d425"
        },
        "id": "cFTdC2FP1toQ"
      },
      "source": [
        "toy_dataset_path = os.path.join(dataset_folder_path, \"criteo_toy_dataset.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "13ad4dd2-c2d8-4c3f-bde2-c5cdd82933d7"
        },
        "id": "edC69MiW1toQ"
      },
      "source": [
        "!head -n 1000000 {dataset_path} > {toy_dataset_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "543acccb-6454-4ffe-960d-0dcbc33894b7"
        },
        "id": "UkCSJvYm1toQ"
      },
      "source": [
        "Let's say we want to perform a basic operation: estimate the number of positive samples within the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e85b4203-6efc-46f0-ba5b-52ebe297b95a"
        },
        "id": "B-J-n_cS1toQ"
      },
      "source": [
        "### Basic approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3d57d10b-15ee-4e3d-82fc-495d40a6b376"
        },
        "id": "O__gbdTl1toR"
      },
      "source": [
        "def compute_positive_label_proportion(dataset_path, columns):\n",
        "    df = pd.read_csv(dataset_path, sep=\"\\t\", header=None, names=columns)\n",
        "    return df['label'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fceb5b2b-9553-44a7-8d83-030409be41d9"
        },
        "id": "njH11mxQ1toR"
      },
      "source": [
        "Let's measure its memory footprint with the `%%memit` magic function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dafb5614-7640-4d36-a0f1-eac939d24f65"
        },
        "id": "C8GTP__01toR"
      },
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion(toy_dataset_path, columns)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "83936c1c-2f35-47df-9daf-0ea7b4036e92"
        },
        "id": "SPcgV4-g1toR"
      },
      "source": [
        "What would happen if you run the same function on a 45 times bigger dataset ?\n",
        "\n",
        "You can give a try with `compute_positive_label_proportion(dataset_path, columns)`... at your own risks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7ba93320-7ce7-4a5c-b36f-f1e9f86a1528"
        },
        "id": "Kfq_TXl21toR"
      },
      "source": [
        "### Specifying column types\n",
        "We can help pandas by specifying the column types to be used such that it does not need to infer it. Do so with the parameter dtype of pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "66f49dc1-bf77-4b33-b4f9-85a8c4f2cbe6"
        },
        "id": "CYdlftV-1toS"
      },
      "source": [
        "col_types = OrderedDict()\n",
        "# Fill col_types here\n",
        "\n",
        "def compute_positive_label_proportion_with_dtype(dataset_path, columns, col_types):\n",
        "    # Read csv with dtype and return positive_label_proportion\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7e275dee-57b0-4e2e-9d14-775b3c07daba"
        },
        "id": "uN3b0P941toS"
      },
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype(toy_dataset_path, columns, col_types)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56287730-4405-4ae2-90a2-cedf29035f80"
        },
        "id": "YsQ2Gnr21toS"
      },
      "source": [
        "### Reading data by chunks\n",
        "We can control the amount of memory we need by loading only a small chunk of the data and processing it before moving to the next chunk.\n",
        "\n",
        "See documentation at https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#iterating-through-files-chunk-by-chunk\n",
        "\n",
        "```\n",
        "with pd.read_csv(..., chunksize=10, nrows=100) as reader:\n",
        "    for chunk in reader:\n",
        "        print(chunk)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d3c192e9-b4aa-4b81-9abc-9258896a506d"
        },
        "id": "FN6c3iU71toS"
      },
      "source": [
        "def compute_positive_label_proportion_with_dtype_and_chunksize(dataset_path, columns, col_types, chunksize):\n",
        "    # Read csv with dtype and chunksize and return positive_label_proportion\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "68a665f7-489e-4484-8cf3-4d376d1ea657"
        },
        "id": "LTNRB5ut1toS"
      },
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype_and_chunksize(toy_dataset_path, columns, col_types, 100_000)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a5625703-6f57-414d-a2f6-25d86f4ae4fb"
        },
        "id": "gPrY41On1toS"
      },
      "source": [
        "This can now be applied to the full dataset with no memory issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9f925158-ac73-4723-be68-249d05c75f5c"
        },
        "id": "_TaCyB6Y1toT"
      },
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype_and_chunksize(dataset_path, columns, col_types, 100_000)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4722389c-73ce-482b-b494-a363950f2276"
        },
        "id": "dcDtv6GV1toT"
      },
      "source": [
        "## Training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f7b3b941-be9a-4dff-ac77-fb980120f091"
        },
        "id": "Og-efo9O1toT"
      },
      "source": [
        "### Split train and test datasets\n",
        "Since the datasets contain one line per example, we can split them into train and test by simply iterating over the lines. For each line in the original dataset: write it to the test data set with a probability p and write it to the train dataset with a probability 1 - p."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6ddfeb48-4ac1-43ef-9c14-ff395c0fcb18"
        },
        "id": "PfhVx11b1toT"
      },
      "source": [
        "def split_train_test(full_dataset_path, train_dataset_path, test_dataset_path, test_ratio, seed=302984, print_every=None):\n",
        "    random.seed(seed)\n",
        "    pass\n",
        "        \n",
        "train_dataset_path = os.path.join(dataset_folder_path, \"criteo_train_dataset.txt\")\n",
        "test_dataset_path = os.path.join(dataset_folder_path, \"criteo_test_dataset.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b433a302-de7a-4045-b67c-f3ebb3aab794"
        },
        "id": "qHx2pfo51toU"
      },
      "source": [
        "if not os.path.exists(train_dataset_path) or not os.path.exists(test_dataset_path):\n",
        "    split_train_test(dataset_path, train_dataset_path, test_dataset_path, test_ratio=0.1, print_every=10_000_000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2faffa05-0de3-401f-b7d4-b87714a98a17"
        },
        "id": "Z4cr2-2R1toU"
      },
      "source": [
        "!wc -l {test_dataset_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b524a5dd-b21f-4572-bf9b-6de10e539122"
        },
        "id": "kqxG7EfA1toU"
      },
      "source": [
        "### Shuffling\n",
        "The convergence guarantees of SGD rely on the fact that the observations come at random. Hence, shuffling between epochs is important.\n",
        "\n",
        "First result of \"How to shuffle a file that is too big for memory\" on Google: https://stackoverflow.com/a/40814865/2015762\n",
        "\n",
        "Note that quicker pseudo-shuffling strategies exists, but this fits our \"Big data on your laptop\" problematic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e8546d50-68d4-4f24-abb1-e0c9aa11df96"
        },
        "id": "L40k3Waq1toU"
      },
      "source": [
        "!awk 'BEGIN{srand();} {printf \"%06d %s\\n\", rand()*1000000, $0;}' /databricks/driver/sync/data/criteo_dataset/criteo_test_dataset.txt | sort -n | cut -c8- > /databricks/driver/sync/data/criteo_dataset/criteo_test_dataset_shuffled.txt\n",
        "# We can run it on the train dataset too but let'ss skip it since it is quite long\n",
        "# !awk 'BEGIN{srand();} {printf \"%06d %s\\n\", rand()*1000000, $0;}' /databricks/driver/sync/data/criteo_dataset/criteo_train_dataset.txt | sort -n | cut -c8- > /databricks/driver/sync/data/criteo_dataset/criteo_train_dataset_shuffled.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d7444e81-90d6-43c5-acf8-68699e77a3dd"
        },
        "id": "CSm7T7KY1toU"
      },
      "source": [
        "### Training\n",
        "In order to train a logistic model on chunks of data, we will use scikit-learn `SGDClassifier` (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) and train for its `log` loss with its `partial_fit` method.\n",
        "We can now apply the previous data processing pipeline and add the training to obtain a trained classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5f300552-af45-4ab0-a495-db4ba9a8a827"
        },
        "id": "hhNSS4WQ1toV"
      },
      "source": [
        "#  To begin with, let's not do any preprocessing and deal with \"ready to use\" continuous features only\n",
        "def preprocess_data(chunk, integer_features, categorical_features):\n",
        "    return chunk[integer_features].fillna(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c2fff3fb-3a17-42b0-b5aa-d05ff9632ae7"
        },
        "id": "PRZmPAag1toV"
      },
      "source": [
        "max_training_steps = 1_000\n",
        "chunk_size = 1_000\n",
        "print_every = 100\n",
        "\n",
        "# 1. Read train data by chunks\n",
        "# 2. Apply preprocess_data to return the continous features\n",
        "# 3. Train classifier on this chunk\n",
        "# 4. Stop after `max_training_steps`\n",
        "\n",
        "classifier = SGDClassifier(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3ccf2390-7b6e-4ec1-b29f-5e3f767e3fbe"
        },
        "id": "284dYqJH1toV"
      },
      "source": [
        "### Testing\n",
        "Let's evaluate the performances of the trained classifier. We should iterate over the test dataset and evaluate the labels predicted by the classifier with `roc_auc_score` and `log_loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9a3f8de1-b8aa-41c8-9a46-9c62122da7f9"
        },
        "id": "AuYFnt3C1toV"
      },
      "source": [
        "max_testing_steps = 100\n",
        "chunk_size = 1_000\n",
        "print_every = 50\n",
        "\n",
        "# 1. Read test data by chunks\n",
        "# 2. Apply preprocess_data to return the continous features\n",
        "# 3. Predict labels with classifiers\n",
        "# 4. Compute AUC score and Log loss for this chunk\n",
        "# 5. Stop after `max_testing_steps`\n",
        "# 6. Return averaged values of the metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f2c9ec3e-554e-4caf-b9bb-bcec65ed3dba"
        },
        "id": "UyHB-XFk1toV"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cacde538-c462-4da5-a5cb-78bbf2e8b624"
        },
        "id": "vKHVqdg-1toW"
      },
      "source": [
        "### Continuous features\n",
        "A smart way to deal with continuous features (counting integer features are part of them), consists in transforming them into categorical features through a quantile transformation. To do so we will use scikit-learn KBinsDiscretizer : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html.\n",
        "\n",
        "It can be used as following\n",
        "```\n",
        "df = pd.DataFrame({'col_1': np.random.normal(size=1000), 'col_2': np.random.poisson(lam=1, size=1000)})\n",
        "bucketizer = KBinsDiscretizer(n_bins=20, encode='ordinal')\n",
        "bucketizer.fit(df)\n",
        "df_bucketized = pd.DataFrame(bucketizer.transform(df), columns=[f'{col}_bucketized' for col in df.columns], index=df.index)\n",
        "sns.jointplot(data=pd.concat((df, df_bucketized), axis=1), x=\"col_1\", y=\"col_1_bucketized\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e87b6181-a50c-4a25-9269-825927d14df4"
        },
        "id": "2EsmGHJA1toW"
      },
      "source": [
        "1. Create a `KBinsDiscretizer` and train it on the first chunk of the dataset\n",
        "1. Update `preprocess_data` to add a bucketize step to the training pipeline. What happens if you change the `encode` parameter?\n",
        "1. Do not forget to deal with missing values, you do not want to carry on NaNs. You can for example replace them with -1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6603e270-56e3-4ed9-a6e3-b6b8b70b1e0b"
        },
        "id": "Bl7A-9Fb1toW"
      },
      "source": [
        "bucketizer = KBinsDiscretizer(...)\n",
        "\n",
        "def preprocess_data(chunk, integer_features, categorical_features):\n",
        "    # To Update\n",
        "    return \n",
        "\n",
        "# To test your bucketization\n",
        "preprocess_data(chunk, integer_features, categorical_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ada99dde-fc5f-41e5-bd37-b7a018e66736"
        },
        "id": "4Jjf5iVp1toW"
      },
      "source": [
        "### Categorical features\n",
        "For categorical features we will implement the hashing trick using scikit-learn FeatureHasher: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html\n",
        "\n",
        "It can be used as following\n",
        "```\n",
        "df = pd.DataFrame({'col_1': np.random.choice(['a', 'b', 'c'], size=100), 'col_2': np.random.poisson(size=100)})\n",
        "hasher = FeatureHasher(n_features=2**16, input_type=\"dict\")\n",
        "hasher.transform((row._asdict() for row in df.itertuples(index=False)))\n",
        "```\n",
        "Again, you will probably want to ensurer you get rid of the NaNs. What value could you set for these?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4598f259-ae84-457e-b565-1fbccd4359e9"
        },
        "id": "nycI1UaO1toW"
      },
      "source": [
        "Let's improve our previous pipeline and apply the hashing trick to the categorical features **and** to the bucketized continuous features (have a look at `pd.concat`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "51d49529-7ca5-4f67-b16e-b8c347ac29b3"
        },
        "id": "boAElGlg1toW"
      },
      "source": [
        "hasher = FeatureHasher(...)\n",
        "\n",
        "def preprocess_data(chunk, integer_features, categorical_features):\n",
        "    # To Update to both bucketize and then hash\n",
        "    return \n",
        "\n",
        "# To test your preprocessing\n",
        "preprocess_data(chunk, integer_features, categorical_features)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}